{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables.\n",
    "- Used in pre-processing step\n",
    "- Take only smaller number of variables while keeping as much information as possible\n",
    "- Reduce overfitting \n",
    "- Obtain independent features\n",
    "- Lower computational intensity\n",
    "- Enable visualization of hyperdimensional features\n",
    "- Compression => Loss of information => loss of performance\n",
    "- Always measure model performance before and after dimensionality reduction\n",
    "- 2 Types:\n",
    "    1. Feature Selection:\n",
    "        - Selecting a subset of existing features without any transformation (based on predictive power)\n",
    "        - Looking for best combination of features (not best individual features)\n",
    "    2. Feature Extraction:\n",
    "        - Involves transformation\n",
    "        - Creating new features from combination of existing features\n",
    "        - 2 types:\n",
    "            1. Linear projections:\n",
    "                - faster, \n",
    "                - deterministic\n",
    "                - Principal Component Analysis (PCA)\n",
    "                - `from sklearn.decomposition import PCA`\n",
    "                - Latent Dirichlet Allocation (LDA)\n",
    "                - `from sklearn.decomposition import LatentDirichletAllocation`\n",
    "            2. Non-Linear projections:\n",
    "                - slower, \n",
    "                - non-deterministic\n",
    "                - Isomap\n",
    "                - `from sklearn.manifold import Isomap`\n",
    "                - t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "                - `from sklearn.manifold import TSNE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Family: Linear methods.\n",
    "- Intuition: Principal components are directions of highest variability in data.\n",
    "- Reduction = keeping only top #N principal components.\n",
    "- Assumption: Data is normally distributed.\n",
    "- Caveat: Very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_dimensions=3)\n",
    "# X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster = Group of entities or events sharing similar attributes\n",
    "- The process of applying Machine Learning algorithms for automatic discovery of clusters .\n",
    "- Very sensitive to pre-processing\n",
    "- Unsupervised Learning\n",
    "- Popular models:\n",
    "    - KMeans clustering:\n",
    "        - No. of clusters must be specified\n",
    "        - Works on linear separation boundaries\n",
    "        - Fast\n",
    "    - Spectral clustering:\n",
    "        - No. of clusters must be specified\n",
    "        - Very non-linear separation boundaries\n",
    "        - Slow\n",
    "    - DBSCAN:\n",
    "        - No. of clusters is figured out by itself\n",
    "        - Non-linear separation boundaries\n",
    "        - Leave too many points belonging to no cluster\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining optimized no of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elbow method: \n",
    "    - we scan a possible number of clusters.\n",
    "    - we measure sum of squared distances (how far each point from center of the cluster )\n",
    "    - We visualize them against each other on a graph\n",
    "    - The last bending before \"almost linear\" point is the optimal number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.jpg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unsupervised (no \"ground truth\", no expectations on how the samples should be clustered )\n",
    "    - Variance Ratio Criterion: \n",
    "        - \"What is the average distance of each point to the center of the cluster AND what is the distance between the clusters?\"\n",
    "        - `sklearn.metrics.calinski_harabaz_score`\n",
    "    - Silhouette score \n",
    "        - \"How close is each point to its own cluster VS how close it is to the others?\"\n",
    "        - `sklearn.metrics.silhouette_score`\n",
    "2. Supervised (\"ground truth\"/expectations provided or evaluating result with supervision)\n",
    "    - Mutual information (MI) criterion: \n",
    "        - `sklearn.metrics.mutual_info_score`\n",
    "    - Homogeneity score: \n",
    "        - `sklearn.metrics.homogeneity_score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detecting unusual entities or events (rare events).\n",
    "- Hard to define what's odd, but possible to define what's normal.\n",
    "- Use cases :\n",
    "    - Credit card fraud detection\n",
    "    - Network security monitoring\n",
    "    - Heart-rate monitoring\n",
    "- Approaches:\n",
    "    - Thresholding : Detecting values that supasses stable values that remain within threshold over time\n",
    "    <center><img src=\"images/05.jpg\"  style=\"width: 200px, height: 200px;\"/></center>\n",
    "\n",
    "    - rate of change : Derivatives within some interval\n",
    "    <center><img src=\"images/06.jpg\"  style=\"width: 200px, height: 200px;\"/></center>\n",
    "\n",
    "    - shape monitoring : inspecting shape of the waveform\n",
    "    <center><img src=\"images/07.jpg\"  style=\"width: 200px, height: 200px;\"/></center>\n",
    "\n",
    "- Common Models:\n",
    "    - Robust covariance \n",
    "        - assumes normal distribution\n",
    "        - `from sklearn.covariance import EllipticEnvelope`\n",
    "    - Isolation Forest \n",
    "        - powerful\n",
    "        - computationally demanding / slower\n",
    "        - `from sklearn.ensemble import IsolationForest`\n",
    "    - One-Class SVM \n",
    "        - sensitive to outliers\n",
    "        - many false negatives\n",
    "        - `from sklearn.svm import OneClassSVM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision = How many of the PREDICTED anomalies I have detected are TRUE anomalies?\n",
    "- Recall = Out of all TRUE anomalies How many of the TRUE anomalies I have managed to detect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Package\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# # Provide model and hyperparameters\n",
    "# algorithm = IsolationForest()\n",
    "\n",
    "# # Fit the model\n",
    "# algorithm.fit(X_train)\n",
    "\n",
    "# # Apply the model and detect the outliers\n",
    "# results = algorithm.predict(X_test)\n",
    "\n",
    "# # Evaluate Model\n",
    "# from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "# confusion_matrix(y_test, y_predicted)\n",
    "# precision_score(y_test, y_predicted)\n",
    "# recall_score(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do we have target variable ?\n",
    "    - Yes (Supervised)\n",
    "        - Target variable a category (Classification)\n",
    "        - Target variable a number (Regression)\n",
    "    - No (Unsupervised)\n",
    "        - Simplify feature space (Dimensionality Reduction)\n",
    "        - Find groups of similar records (Clustering) \n",
    "        - Novelty inside data (Anomaly Detection)\n",
    "\n",
    "- Priority: (Simplicity First, move up complexity ladder as needed)\n",
    "    - Interpretable models\n",
    "        - Simple\n",
    "        - Linear regression (Linear, Logistic, Lasso, Ridge)\n",
    "        - Decision Trees\n",
    "    - Well performing models\n",
    "        - Complex\n",
    "        - Tree ensembles (Random Forests, Gradient Boosted Trees)\n",
    "        - Artificial Neural Networks\n",
    "\n",
    "- Multiple Metrics:\n",
    "    - Satisfying metrics:\n",
    "        - Cut-off criteria that every candidate model needs to meet.\n",
    "        - e.g. minimum accuracy, maximum execution time, etc\n",
    "        - Multiple metric possible\n",
    "    - Optimizing metrics\n",
    "        - Ultimate priority \n",
    "        - e.g. \"minimize false positives\", \"maximize recall\"\n",
    "        - \"There can be only one\"\n",
    "    - Final model:\n",
    "        - Passes the bar on all satisfying metrics \n",
    "        - The best score on the optimization metric.\n",
    "- Interpretation\n",
    "    - Global\n",
    "        - Decision-making rules \n",
    "        - Common approaches:\n",
    "            - Visualizing Decision tree\n",
    "            - plot Feature importance \n",
    "    - Local\n",
    "        - \"Why was this datapoint classified in this way?\"\n",
    "        - LIME algorithm (Local Interpretable Model-Agnostic Explanations)\n",
    "- Data always changes, so does the model. The whole process is thus repeated time-to-time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
